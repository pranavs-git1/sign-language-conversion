# **Sign Language Converter**

A computer vision–based project that converts sign language gestures into text, making communication more inclusive and accessible for people with hearing or speech impairments.

# Features

- Real-time gesture recognition using OpenCV

- Machine Learning integration for accurate prediction of hand signs

- Achieved 90%+ accuracy on custom-trained gesture dataset

- Converts recognized gestures into readable text for easy communication

- Modular design – can be extended to voice output or more gestures

# Tech Stack

Languages: Python

Libraries/Frameworks: OpenCV, NumPy, TensorFlow/Keras, Scikit-learn

Tools: Jupyter Notebook, PyCharm

# How It Works

Capture hand gestures using a webcam.

Preprocess images (grayscale, thresholding, contour extraction).

Extract features and classify gestures using a trained ML model.

Display the recognized gesture as text in real-time.

# Results

Accuracy: 90%+ on validation set

Supports: Multiple sign gestures (expandable with new data)

Future Scope: Add voice output, support for dynamic gestures

# Applications

Accessibility tool for hearing-impaired individuals

Real-time communication bridge between signers and non-signers

Can be extended into mobile apps or IoT-based solutions

# Contributing

Pull requests are welcome! If you’d like to add new gestures, improve accuracy, or extend functionality, feel free to contribute.

# License

This project is licensed under the MIT License – free to use and modify.

# Author

Pranav Swarnkar
